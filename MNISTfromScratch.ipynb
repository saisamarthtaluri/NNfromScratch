{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f9cc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Train Accuracy: 82.44%, Val Accuracy: 82.04%\n",
      "Epoch 200/1000, Train Accuracy: 89.28%, Val Accuracy: 89.58%\n",
      "Epoch 300/1000, Train Accuracy: 92.03%, Val Accuracy: 91.67%\n",
      "Epoch 400/1000, Train Accuracy: 93.09%, Val Accuracy: 92.60%\n",
      "Epoch 500/1000, Train Accuracy: 93.91%, Val Accuracy: 93.44%\n",
      "Epoch 600/1000, Train Accuracy: 94.64%, Val Accuracy: 93.88%\n",
      "Epoch 700/1000, Train Accuracy: 95.12%, Val Accuracy: 94.31%\n",
      "Epoch 800/1000, Train Accuracy: 95.59%, Val Accuracy: 94.60%\n",
      "Epoch 900/1000, Train Accuracy: 95.94%, Val Accuracy: 94.90%\n",
      "Epoch 1000/1000, Train Accuracy: 96.30%, Val Accuracy: 95.05%\n",
      "Predicted labels: [5 7 5 ... 7 6 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 31 #Why 31? It's my birthday xd\n",
    "\n",
    "def load_data(train_file, test_size=0.2, random_state=None):\n",
    "    # Load the MNIST dataset\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    \n",
    "    X = train_data.drop('label', axis=1).values\n",
    "    y = train_data['label'].values\n",
    "    \n",
    "    # Normalize the input features\n",
    "    X = X / 255.0\n",
    "    \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_samples = int(num_samples * test_size)\n",
    "    train_indices = indices[test_samples:]\n",
    "    test_indices = indices[:test_samples]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = labels.shape[0]\n",
    "    one_hot = np.zeros((num_samples, num_classes))\n",
    "    one_hot[np.arange(num_samples), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Initialize weights and biases using Xavier initialization\n",
    "def initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2 / (input_size + hidden_size1))\n",
    "    b1 = np.zeros(hidden_size1)\n",
    "\n",
    "    W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2 / (hidden_size1 + hidden_size2))\n",
    "    b2 = np.zeros(hidden_size2)\n",
    "\n",
    "    W3 = np.random.randn(hidden_size2, hidden_size3) * np.sqrt(2 / (hidden_size2 + hidden_size3))\n",
    "    b3 = np.zeros(hidden_size3)\n",
    "\n",
    "    W4 = np.random.randn(hidden_size3, output_size) * np.sqrt(2 / (hidden_size3 + output_size))\n",
    "    b4 = np.zeros(output_size)\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1, 'b1': b1,\n",
    "        'W2': W2, 'b2': b2,\n",
    "        'W3': W3, 'b3': b3,\n",
    "        'W4': W4, 'b4': b4\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the softmax activation function\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    W3, b3 = parameters['W3'], parameters['b3']\n",
    "    W4, b4 = parameters['W4'], parameters['b4']\n",
    "    \n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = relu(z2)\n",
    "    \n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = relu(z3)\n",
    "    \n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    a4 = softmax(z4)\n",
    "    \n",
    "    cache = {\n",
    "        'a1': a1, 'z1': z1,\n",
    "        'a2': a2, 'z2': z2,\n",
    "        'a3': a3, 'z3': z3,\n",
    "        'a4': a4, 'z4': z4\n",
    "    }\n",
    "    \n",
    "    return a4, cache\n",
    "\n",
    "# Backpropagation\n",
    "def backward_propagation(X, y, cache, parameters):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    a1, z1 = cache['a1'], cache['z1']\n",
    "    a2, z2 = cache['a2'], cache['z2']\n",
    "    a3, z3 = cache['a3'], cache['z3']\n",
    "    a4, z4 = cache['a4'], cache['z4']\n",
    "    \n",
    "    delta4 = a4 - y\n",
    "    delta3 = np.dot(delta4, parameters['W4'].T) * (z3 > 0).astype(float)\n",
    "    delta2 = np.dot(delta3, parameters['W3'].T) * (z2 > 0).astype(float)\n",
    "    delta1 = np.dot(delta2, parameters['W2'].T) * (z1 > 0).astype(float)\n",
    "    \n",
    "    dW4 = np.dot(a3.T, delta4) / m\n",
    "    db4 = np.sum(delta4, axis=0) / m\n",
    "    \n",
    "    dW3 = np.dot(a2.T, delta3) / m\n",
    "    db3 = np.sum(delta3, axis=0) / m\n",
    "    \n",
    "    dW2 = np.dot(a1.T, delta2) / m\n",
    "    db2 = np.sum(delta2, axis=0) / m\n",
    "    \n",
    "    dW1 = np.dot(X.T, delta1) / m\n",
    "    db1 = np.sum(delta1, axis=0) / m\n",
    "    \n",
    "    gradients = {\n",
    "        'dW4': dW4, 'db4': db4,\n",
    "        'dW3': dW3, 'db3': db3,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW1': dW1, 'db1': db1\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    parameters['W4'] -= learning_rate * gradients['dW4']\n",
    "    parameters['b4'] -= learning_rate * gradients['db4']\n",
    "    \n",
    "    parameters['W3'] -= learning_rate * gradients['dW3']\n",
    "    parameters['b3'] -= learning_rate * gradients['db3']\n",
    "    \n",
    "    parameters['W2'] -= learning_rate * gradients['dW2']\n",
    "    parameters['b2'] -= learning_rate * gradients['db2']\n",
    "    \n",
    "    parameters['W1'] -= learning_rate * gradients['dW1']\n",
    "    parameters['b1'] -= learning_rate * gradients['db1']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(X, y, parameters):\n",
    "    a4, _ = forward_propagation(X, parameters)\n",
    "    predicted_labels = np.argmax(a4, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == y) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size, hidden_size1, hidden_size2, hidden_size3, output_size,\n",
    "                learning_rate, num_epochs):\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_classes = output_size\n",
    "    \n",
    "    y_train_encoded = one_hot_encode(y_train, num_classes)\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        a4, cache = forward_propagation(X_train, parameters)\n",
    "        \n",
    "        gradients = backward_propagation(X_train, y_train_encoded, cache, parameters)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            train_accuracy = calculate_accuracy(X_train, y_train, parameters)\n",
    "            val_accuracy = calculate_accuracy(X_val, y_val, parameters)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Test the model\n",
    "def predict(X_test, parameters):\n",
    "    a4, _ = forward_propagation(X_test, parameters)\n",
    "    predicted_labels = np.argmax(a4, axis=1)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# Path to train.csv\n",
    "train_file = 'train.csv'\n",
    "\n",
    "# Load the MNIST dataset and split into train, validation, and test sets\n",
    "X_train, y_train, X_val, y_val = load_data(train_file)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "hidden_size3 = 16\n",
    "output_size = 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "\n",
    "# Train the model\n",
    "parameters = train_model(X_train, y_train, X_val, y_val, input_size, hidden_size1, hidden_size2, hidden_size3, output_size,\n",
    "                         learning_rate, num_epochs)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = predict(X_test, parameters)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83d80878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_samples(X, y_true, y_pred, num_samples=5):\n",
    "    sample_indices = np.random.choice(len(X), size=num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(10, 4))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        image = X[idx].reshape(28, 28)\n",
    "        true_label = y_true[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        \n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd3ae907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predicted_labels = predict(X_test, parameters)\n",
    "\n",
    "# Display a few samples with their predicted and true values\n",
    "display_samples(X_test, y_test, predicted_labels, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8eb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
